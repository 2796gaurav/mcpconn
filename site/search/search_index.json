{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-mcpconn","title":"Welcome to mcpconn","text":"<p>mcpconn is a Python library that provides a simple and efficient way to connect your applications to AI models using the Model Context Protocol (MCP). It acts as a wrapper around the <code>mcp</code> library, offering a streamlined client interface for seamless integration with various AI providers and transport protocols.</p> <p> </p>"},{"location":"#features","title":"\u2728 Features","text":"<ul> <li>Simplified Client Interface: A high-level <code>mcpconn</code> for easy interaction with MCP servers.</li> <li>Multi-provider Support: Out-of-the-box support for Anthropic and OpenAI models.</li> <li>Flexible Transports: Connect to servers using STDIO, SSE, or Streamable HTTP.</li> <li>Built-in Guardrails: Protect your application with content filtering, PII masking, and injection detection.</li> <li>Conversation Management: Easily manage conversation history, context, and persistence.</li> <li>Asynchronous by Design: Built with <code>asyncio</code> for high-performance, non-blocking I/O.</li> <li>Extensible: Easily add new LLM providers, transports, or guardrails.</li> </ul> <p>Ready to dive in? Check out the Getting Started guide. </p>"},{"location":"api-reference/","title":"API Reference","text":"<p>This page provides a reference for the <code>mcpconn</code> library's public API.</p>"},{"location":"api-reference/#mcpconn","title":"<code>mcpconn</code>","text":"<p>The <code>mcpconn</code> is the main entry point for interacting with MCP servers.</p> <p>::: mcpconn.MCPClient     options:       show_root_heading: true       show_source: false</p>"},{"location":"api-reference/#__init__","title":"<code>__init__</code>","text":"<p>Initializes the MCP client.</p> <p>Parameters:</p> <ul> <li><code>llm_provider</code> (str): The LLM provider to use. Currently supports <code>\"anthropic\"</code> and <code>\"openai\"</code>. Defaults to <code>\"anthropic\"</code>.</li> <li><code>env_file</code> (str, optional): Path to a <code>.env</code> file to load environment variables from.</li> <li><code>timeout</code> (float): Default timeout in seconds for operations. Defaults to <code>30.0</code>.</li> <li><code>conversation_id</code> (str, optional): An existing conversation ID to resume.</li> <li><code>auto_generate_ids</code> (bool): Whether to automatically generate a unique conversation ID for each message if one isn't active. Defaults to <code>True</code>.</li> <li><code>**llm_kwargs</code>: Additional keyword arguments to pass to the LLM provider's constructor.</li> </ul>"},{"location":"api-reference/#connect","title":"<code>connect</code>","text":"<p>Connects to an MCP server.</p> <p>Parameters:</p> <ul> <li><code>connection_string</code> (str): The connection string for the server (e.g., a URL for HTTP, or a command for STDIO). For Python scripts, do not include the 'python' prefix; it is added automatically for <code>.py</code> files.</li> <li><code>transport</code> (str, optional): The transport protocol to use (<code>\"stdio\"</code>, <code>\"sse\"</code>, <code>\"http\"</code>). If <code>None</code>, it's inferred from the connection string.</li> <li><code>headers</code> (dict, optional): A dictionary of headers to use for HTTP-based transports.</li> </ul>"},{"location":"api-reference/#query","title":"<code>query</code>","text":"<p>Sends a message to the AI and gets a response.</p> <p>Parameters:</p> <ul> <li><code>message</code> (str): The message to send.</li> <li><code>max_iterations</code> (int): The maximum number of tool-use iterations to perform. Defaults to <code>5</code>.</li> <li><code>conversation_id</code> (str, optional): The ID of the conversation to use for this query.</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: The AI's response.</li> </ul>"},{"location":"api-reference/#conversation-management","title":"Conversation Management","text":""},{"location":"api-reference/#start_conversation","title":"<code>start_conversation</code>","text":"<p>Starts a new conversation or resumes an existing one.</p> <p>Parameters:</p> <ul> <li><code>conversation_id</code> (str, optional): The ID of the conversation to start or resume. If <code>None</code>, a new one is generated.</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: The active conversation ID.</li> </ul>"},{"location":"api-reference/#get_conversation_history","title":"<code>get_conversation_history</code>","text":"<p>Retrieves the message history for the current conversation.</p> <p>Returns:</p> <ul> <li><code>list</code>: A list of message dictionaries.</li> </ul>"},{"location":"api-reference/#save_conversation-load_conversation","title":"<code>save_conversation</code> / <code>load_conversation</code>","text":"<p>Saves the current conversation state to a file or loads it from a file.</p> <p>Parameters:</p> <ul> <li><code>filepath</code> (str): The path to the file.</li> </ul>"},{"location":"api-reference/#guardrails","title":"Guardrails","text":""},{"location":"api-reference/#add_guardrail","title":"<code>add_guardrail</code>","text":"<p>Adds a guardrail to the client for content moderation.</p> <p>Parameters:</p> <ul> <li><code>guardrail</code>: An instance of a guardrail class (e.g., <code>WordMaskGuardrail</code>). </li> </ul>"},{"location":"code-of-conduct/","title":"Code of Conduct","text":""},{"location":"code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that are professional, respectful, and welcoming.</p>"},{"location":"code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code-of-conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at 2796gaurav@gmail.com. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code-of-conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code-of-conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code-of-conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interaction in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code-of-conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code-of-conduct/#enforcement-guidelines_1","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code-of-conduct/#1-correction_1","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code-of-conduct/#2-warning_1","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interaction in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code-of-conduct/#3-temporary-ban_1","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code-of-conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p>"},{"location":"core-concepts/","title":"Core Concepts","text":"<p>This page explains the key concepts behind <code>mcpconn</code> and the protocol it is built upon.</p>"},{"location":"core-concepts/#the-model-context-protocol-mcp","title":"The Model Context Protocol (MCP)","text":"<p>At its heart, <code>mcpconn</code> is designed to simplify interactions with servers that use the Model Context Protocol (MCP). MCP is a standardized protocol that creates a common ground for different AI models (like those from Anthropic or OpenAI) and the applications that use them.</p> <p>Think of it as a universal adapter for AI. Instead of writing custom code to handle the specific API of each AI provider, you can connect to an MCP-compliant server that handles those details for you.</p> <p>The key benefits of this approach are:</p> <ul> <li>Provider Agnostic: You can switch between different AI providers with minimal code changes, often just by changing a parameter in the client.</li> <li>Standardized Tool Use: MCP defines a standard way for AI models to request the use of external tools (like a weather API, a calculator, or a database). Your application can expose these tools to the AI in a consistent way.</li> <li>Simplified Communication: The protocol handles the complexities of streaming responses, managing conversation history, and handling different data formats.</li> </ul>"},{"location":"core-concepts/#how-mcpconn-helps","title":"How <code>mcpconn</code> Helps","text":"<p>While MCP provides the standard, <code>mcpconn</code> provides the convenience. It acts as a high-level client library that abstracts away the low-level details of the protocol.</p> <p>Instead of manually constructing MCP messages, you can use the intuitive methods on the <code>mcpconn</code>:</p> <ul> <li><code>client.connect()</code>: Handles establishing the connection over different transports (like STDIO or HTTP).</li> <li><code>client.query()</code>: Sends a message to the AI and automatically handles the back-and-forth of tool usage.</li> <li><code>client.start_conversation()</code>: Manages session IDs and history.</li> </ul> <p>By using <code>mcpconn</code>, you can focus on building your application's logic instead of worrying about the intricacies of AI integration. </p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get up and running with <code>mcpconn</code>.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>To install <code>mcpconn</code>, run the following command in your terminal:</p> <pre><code>pip install mcpconn\n</code></pre> <p>This will install the core library and its dependencies.</p>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":"<p>Here's a simple example of how to use <code>mcpconn</code> to connect to an MCP server and interact with an AI model:</p> <pre><code>import asyncio\nfrom mcpconn import MCPClient\n\nasync def main():\n    # Set your OpenAI API key in the environment before running\n    # export OPENAI_API_KEY=\"your-key-here\"\n\n    # Connect to a remote MCP server using OpenAI and streamable_http transport\n    # NOTE: OpenAI only supports remote MCP endpoints (not local/stdio/localhost). See: https://platform.openai.com/docs/guides/tools-remote-mcp\n    client = MCPClient(llm_provider=\"openai\")\n    await client.connect(\"https://mcp.deepwiki.com/mcp\", transport=\"streamable_http\")\n\n    # Send a message and get a response\n    response = await client.query(\"give me list of tools provided\")\n    print(f\"AI: {response}\")\n\n    # Disconnect from the server\n    await client.disconnect()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Note: Set your OpenAI API key in the environment before running the example (e.g., <code>export OPENAI_API_KEY=\"your-key-here\"</code>).</p> <p>Warning: OpenAI provider only supports remote MCP endpoints. Local/STDIO/localhost servers are not supported. See: https://platform.openai.com/docs/guides/tools-remote-mcp</p> <p>This is the easiest way to get started: just connect to a remote MCP server using OpenAI and streamable_http transport.</p> <p>Note: For Python scripts, do not include the 'python' prefix in the connection string; it is added automatically by mcpconn.</p> <p>For more detailed examples, please refer to the <code>examples</code> directory in the project repository. </p>"},{"location":"getting-started/#example-servers-for-local-testing","title":"Example Servers for Local Testing","text":"<p>The <code>examples/simple_server/</code> directory contains sample MCP server implementations (stdio, HTTP, SSE) for local testing and development. These are not production servers, but are useful for experimenting with the client examples.</p> <ul> <li><code>weather_stdio.py</code>: Stdio-based weather server</li> <li><code>streamable_http_server.py</code>: HTTP server for weather tools</li> <li><code>sse_mcp_example_server.py</code>: SSE server for weather tools</li> </ul> <p>To run a server for local testing:</p> <pre><code>python examples/simple_server/weather_stdio.py\n# or\npython examples/simple_server/streamable_http_server.py --port 8123\n# or\npython examples/simple_server/sse_mcp_example_server.py --port 8080\n</code></pre> <p>Then connect with a client, e.g.:</p> <pre><code>python examples/simple_client/simple_client.py /path/to/server\n</code></pre> <p>See the <code>examples/simple_client/README.md</code> for more details on client usage and options. </p>"},{"location":"getting-started/#client-usage-examples","title":"Client Usage Examples","text":""},{"location":"getting-started/#switching-models-and-providers","title":"Switching Models and Providers","text":"<p>You can easily switch between Anthropic and OpenAI providers, and specify models:</p> <pre><code># Use Anthropic (default)\npython examples/simple_client/simple_client.py http://localhost:8000\n\n# Use OpenAI (remote MCP only)\npython examples/simple_client/simple_client.py https://mcp.deepwiki.com/mcp --provider openai\n\n# Use a specific model\npython examples/simple_client/simple_client.py http://localhost:8000 --model claude-3-5-sonnet-20241022\n</code></pre> <p>Note: OpenAI only supports remote MCP endpoints (not local/stdio/localhost).</p>"},{"location":"getting-started/#using-conversation-ids-and-history","title":"Using Conversation IDs and History","text":"<p>For advanced conversation management, use the conversation client:</p> <pre><code># Auto-generate conversation IDs\npython examples/simple_client/simple_client_with_conversation_id.py /path/to/server --auto-generate\n\n# Start with a specific conversation ID\npython examples/simple_client/simple_client_with_conversation_id.py /path/to/server --conversation-id \"my-convo-123\"\n</code></pre> <p>In the client, use: - <code>new</code> \u2014 Start a new conversation - <code>history</code> \u2014 Show conversation history - <code>exit</code> \u2014 Quit the client</p>"},{"location":"getting-started/#switching-transports","title":"Switching Transports","text":"<p>Choose the transport that matches your server:</p> <pre><code># Use stdio (local server)\npython examples/simple_client/simple_client.py /path/to/server --transport stdio\n\n# Use SSE\npython examples/simple_client/simple_client.py http://localhost:8000 --transport sse\n\n# Use streamable HTTP\npython examples/simple_client/simple_client.py http://localhost:8000 --transport streamable_http\n</code></pre>"},{"location":"getting-started/#guardrails-content-safety","title":"Guardrails (Content Safety)","text":"<p>For content filtering and safety, use the guardrails client:</p> <pre><code># Enable all guardrails\npython examples/simple_client/simple_client_with_guardrails.py /path/to/server --enable-all\n\n# Enable specific guardrails\npython examples/simple_client/simple_client_with_guardrails.py /path/to/server --enable-word-mask --enable-pii\n</code></pre> <p>See the Using Guardrails page for more details and advanced usage.</p> <p>For more detailed examples and advanced usage, see the <code>examples/simple_client/README.md</code> in the project repository. </p>"},{"location":"guardrails/","title":"Using Guardrails","text":"<p><code>mcpconn</code> comes with a powerful, built-in guardrail system to help you secure your application and moderate content. Guardrails can inspect tool results automatically, and you can also manually apply them to user input or LLM output if desired.</p>"},{"location":"guardrails/#how-it-works","title":"How it Works","text":"<p>The <code>mcpconn</code> client has a <code>GuardrailManager</code> that can hold multiple guardrails. You can add any of the built-in guardrails or even create your own.</p> <p>When you call <code>client.query()</code>, the following happens: 1. The user's message is sent directly to the LLM (no guardrails are applied automatically to user input). 2. If the LLM calls a tool, the tool's result is checked against all registered guardrails before being returned to the user (guardrails are automatically applied to tool results). 3. The LLM's direct output (text) is not automatically checked by guardrails. If you want to filter or mask LLM output, you must do so manually after receiving the response.</p>"},{"location":"guardrails/#basic-example-manually-applying-guardrails-to-llm-output","title":"Basic Example: Manually Applying Guardrails to LLM Output","text":"<pre><code>import asyncio\nfrom mcpconn import MCPClient\nfrom mcpconn.guardrails import PIIGuardrail, WordMaskGuardrail\n\nasync def main():\n    client = MCPClient(llm_provider=\"anthropic\")\n    client.add_guardrail(PIIGuardrail(name=\"pii_detector\"))\n    client.add_guardrail(WordMaskGuardrail(name=\"word_mask\", words_to_mask=[\"secret\"], replacement=\"[CENSORED]\"))\n\n    await client.connect(\"your_server_here\", transport=\"stdio\")\n\n    user_input = \"what is the weather alert in texas.\"\n    # Send to LLM (no guardrails applied automatically)\n    response = await client.query(user_input)\n\n    # If you want to apply guardrails to the LLM output, do it manually:\n    guardrail_results = await client.guardrails.check_all(response)\n    for result in guardrail_results:\n        if not result.passed and result.masked_content:\n            response = result.masked_content\n\n    print(\"Sanitized response:\", response)\n    await client.disconnect()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guardrails/#available-guardrails","title":"Available Guardrails","text":""},{"location":"guardrails/#piiguardrail","title":"<code>PIIGuardrail</code>","text":"<p>Detects and masks common Personally Identifiable Information (PII). - Finds: Email addresses, phone numbers, SSNs, and credit card numbers. - Action: Replaces found PII with <code>[REDACTED]</code>.</p>"},{"location":"guardrails/#wordmaskguardrail","title":"<code>WordMaskGuardrail</code>","text":"<p>Masks a custom list of words or phrases. - Configuration:   - <code>words_to_mask</code>: A list of strings to find.   - <code>replacement</code>: The string to replace them with. - Action: Replaces found words with the replacement string.</p>"},{"location":"guardrails/#responseblockguardrail","title":"<code>ResponseBlockGuardrail</code>","text":"<p>Blocks a response entirely if it contains certain words and replaces it with a standardized message. This is most useful for moderating AI output. - Configuration:   - <code>blocked_words</code>: A list of strings that will trigger the block.   - <code>standardized_response</code>: The message to return instead. - Action: If a blocked word is found, the original response is discarded and the standardized response is returned.</p>"},{"location":"guardrails/#injectionguardrail","title":"<code>InjectionGuardrail</code>","text":"<p>Detects common patterns associated with injection attacks. - Finds: Cross-Site Scripting (XSS), SQL injection, shell injection, and path traversal patterns. - Action: Fails the check if a potential attack is detected. Does not mask content by default, as the entire input should likely be rejected.</p>"},{"location":"guardrails/#where-are-guardrails-enforced","title":"Where Are Guardrails Enforced?","text":"<p>Guardrails in <code>mcpconn</code> are enforced on the client side. This means: - You can add, configure, and manage guardrails in your client code. - The server (including remote MCP servers like OpenAI or Anthropic endpoints) does not enforce guardrails or content filtering by default. - Guardrails are automatically applied only to tool results. LLM output and user input are not filtered unless you do so manually.</p>"},{"location":"guardrails/#openai-and-remote-mcp-servers","title":"OpenAI and Remote MCP Servers","text":"<p>When using a remote MCP server (such as OpenAI via <code>https://mcp.deepwiki.com/mcp</code>), guardrails in your client will only filter tool results (if any). They do not filter or block the LLM's direct output.</p> <p>Warning: OpenAI provider only supports remote MCP endpoints. Local/STDIO/localhost servers are not supported. See: https://platform.openai.com/docs/guides/tools-remote-mcp</p> <p>If you want to apply guardrails to the LLM's output, you can manually check the response after calling <code>client.query()</code>:</p> <pre><code>response = await client.query(user_input)\n# Manually check LLM output with guardrails\nresults = await client.guardrails.check_all(response)\nfor result in results:\n    if not result.passed and result.masked_content:\n        response = result.masked_content\nprint(response)\n</code></pre> <p>This approach allows you to enforce guardrails on any LLM output, regardless of provider or transport.</p> <p>This design allows each client to choose its own safety and filtering policies, rather than relying on the server to enforce them.</p>"},{"location":"guardrails/#advanced-example-guardrails-with-tool-results-and-chat-loop","title":"Advanced Example: Guardrails with Tool Results and Chat Loop","text":"<p>See <code>examples/simple_client/simple_client_with_guardrails.py</code> in the repository for a more advanced example that demonstrates enabling and testing all guardrail types, including a chat loop and tool result filtering. </p>"},{"location":"license/","title":"License","text":"<pre><code>MIT License\n\nCopyright (c) 2024 Gaurav Chauhan\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"}]}